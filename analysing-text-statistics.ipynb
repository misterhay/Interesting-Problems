{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fmisterhay%2FInteresting-Problems&branch=master&subPath=analysing-text-statistics.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Text Statistics\n",
    "\n",
    "Let's try out some statistical analysis of text, including [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), using a [public domain](https://en.wikipedia.org/wiki/Public_domain) book from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    "The example we'll use is the [most downloaded](http://www.gutenberg.org/browse/scores/top) book, [Pride and Prejudice by Jane Austen](http://www.gutenberg.org/ebooks/1342). Running this first code cell will import and display the contents of the book.\n",
    "\n",
    "Feel free to change the link in the following code cell if you'd like to explore another book, but make sure you are using the `Plain Text UTF-8` link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_text_link = 'http://gutenberg.org/files/1342/1342-0.txt'\n",
    "\n",
    "import requests\n",
    "r = requests.get(gutenberg_text_link)\n",
    "r.encoding = 'utf-8'\n",
    "text = r.text.split('***')[2]\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a DataFrame\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text and length of each chapter (number of characters, including spaces and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "for chapter in text.split('Chapter'):\n",
    "    if len(chapter)>500:\n",
    "        chapter = chapter.replace('\\r','').replace('\\n','')\n",
    "        df = df.append({'Chapter Text':chapter, 'Length':len(chapter)}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "From that dataframe we can create a bar graph of chapter lengths using the [cufflinks](https://github.com/santosjorge/cufflinks) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "df.iplot(kind='bar', y='Length', title='Chapter Lengths', yTitle='Length (characters)', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words by Type\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text. For this example we'll just look at adjectives, verbs, nouns, and proper nouns, but you can add to the list on the first line in the code cell.\n",
    "\n",
    "This will take a while to run, and will result in a dataframe containing the number of each of those parts of speech in each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_types = ['ADJ', 'VERB', 'NOUN', 'PROPN'] # https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "#!pip install spacy --user\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "parts_of_speech_df = pd.DataFrame(columns=word_types)\n",
    "for i in range(len(df)):\n",
    "    parts_of_speech_list = []\n",
    "    for token in nlp(df['Chapter Text'][i]):\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types:\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    word_type_count = {}\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:parts_of_speech_list.count(word_type)})\n",
    "    parts_of_speech_df = parts_of_speech_df.append(word_type_count, ignore_index=True)\n",
    "parts_of_speech_df['Chapter'] = parts_of_speech_df.index+1\n",
    "parts_of_speech_df.set_index('Chapter')\n",
    "parts_of_speech_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Verbs\n",
    "\n",
    "To get an idea of the most common words in the text we can look at a part of speech, verbs for example, and count which are the most frequent.\n",
    "\n",
    "This will also take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type = 'VERB'\n",
    "\n",
    "from collections import Counter\n",
    "words_df = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    word_list = []\n",
    "    for token in nlp(df['Chapter Text'][i]):\n",
    "        if token.pos_ == word_type:\n",
    "            word_list.append(token.lemma_.strip().lower())\n",
    "    words_df = words_df.append(Counter(word_list), ignore_index=True)\n",
    "words_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example text there are 1233 unique verbs. Let's look at the `10` most common verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose a verb and plot its frequency by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'say'\n",
    "\n",
    "words_df.iplot(y=verb, title='Frequency of the Word \"'+word+'\"', yTitle='Frequency', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Names\n",
    "\n",
    "We can also look at character names and how often they occur in each chapter. The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    names_list = []\n",
    "    for token in nlp(df['Chapter'][i]):\n",
    "        #if token.pos_ == 'PROPN':\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            names_list.append(token.text)\n",
    "    names_df = names_df.append(Counter(names_list), ignore_index=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Character Names\n",
    "\n",
    "We can check out the list of words identified as names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names_df.columns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "If you'd like to remove columns that are likely categorized incorrectly, we can drop columns with only a few occurrences (fewer than five)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in names_df.columns:\n",
    "    if names_df[column].sum() < 5:\n",
    "        names_df.drop(columns=column, inplace=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Name Frequencies\n",
    "\n",
    "Let's make a bar graph of the top `20` most frequently mentioned characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_noun_df.sum().sort_values(ascending=False).head(20).iplot(kind='bar', yTitle='Frequency', title='Character Names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize how often the top `3` character names are mentioned per chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_character_names = proper_noun_df.sum().sort_values(ascending=False).head(3).index\n",
    "main_characters = proper_noun_df[main_character_names]\n",
    "main_characters.iplot(yTitle='Frequency', xTitle='Chapter', title='Frequency of Character Mentions by Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive).\n",
    "\n",
    "For this we will use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, then visualize the positive and negative sentiment by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment --user\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_df = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    senti = analyzer.polarity_scores(df['Chapter'][i])\n",
    "    sentiment_df = sentiment_df.append(senti, ignore_index=True)\n",
    "sentiment_df.iplot(y=['neg', 'pos'], title='Sentiment Analysis by Chapter', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "One last library to introduce, [textstat](https://github.com/shivam5992/textstat) for checking the readability, complexity, and grade level of text. It includes a [number of functions](https://github.com/shivam5992/textstat#list-of-functions), but we'll only use a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user textstat\n",
    "import textstat\n",
    "readability = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    text = df['Chapter Text'][i]\n",
    "    readability_data = {'Flesch-Kincaid Grade':textstat.flesch_kincaid_grade(text),\n",
    "                        'Gunning Fog Index':textstat.gunning_fog(text),\n",
    "                        'Linsear Write Formula':textstat.linsear_write_formula(text),\n",
    "                        'Readability':textstat.text_standard(text, float_output=True)}\n",
    "    #print(readability_data)\n",
    "    readability = readability.append(readability_data, ignore_index=True)\n",
    "readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe of readability information, we can describe the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully this was an interesting introduction to text statistics. You can also analyse the text from any other online document using similar code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
